{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "951701e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d1d801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config chemins\n",
    "\n",
    "POLLUTION_JSON_PATH = \"pollution.json\"\n",
    "GREEN_CSV_PATH = \"green_areas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa03edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers de nettoyage / normalisation\n",
    "\n",
    "def normalize_str(s):\n",
    "    \"\"\"minuscule + trim + enlever NBSP + normaliser accents + compacter espaces\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\xa0\", \" \")     # NBSP -> espace\n",
    "    s = s.strip()\n",
    "    # Normalisation unicode + suppression des accents\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    # Remplacer espaces multiples par un seul\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.lower()\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    \"russian federation\": \"russia\",\n",
    "    \"united states of america\": \"united states\",\n",
    "    \"united states\": \"united states\",\n",
    "    \"u.s.a.\": \"united states\",\n",
    "    \"great britain\": \"united kingdom\",\n",
    "    \"uk\": \"united kingdom\",\n",
    "    # ajouter d'autres cas si on en voit\n",
    "}\n",
    "\n",
    "def harmonize_country_norm(cn):\n",
    "    if cn is None:\n",
    "        return None\n",
    "    return COUNTRY_MAP.get(cn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfe8ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pollution JSON ===\n",
      "Nb de lignes : 23035\n",
      "         country_name         city_name\n",
      "0  Russian Federation        Praskoveya\n",
      "1              Brazil  Presidente Dutra\n",
      "2               Italy   Priolo Gargallo\n",
      "3              Poland         Przasnysz\n",
      "4              France          Punaauia \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger pollution (JSON facelift)\n",
    "\n",
    "with open(POLLUTION_JSON_PATH, encoding=\"utf-8\") as f:\n",
    "    pollution = json.load(f)\n",
    "\n",
    "df_poll = pd.json_normalize(pollution[\"measurements\"])\n",
    "\n",
    "# Normalisation pays / villes pour jointure\n",
    "df_poll[\"country_norm\"] = df_poll[\"country_name\"].apply(normalize_str)\n",
    "df_poll[\"city_norm\"] = df_poll[\"city_name\"].apply(normalize_str)\n",
    "df_poll[\"country_join\"] = df_poll[\"country_norm\"].apply(harmonize_country_norm)\n",
    "\n",
    "print(\"=== Pollution JSON ===\")\n",
    "print(\"Nb de lignes :\", len(df_poll))\n",
    "print(df_poll[[\"country_name\", \"city_name\"]].head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "549e302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Green areas (raw) ===\n",
      "Nb de lignes : 667\n",
      "Colonnes : ['Country or Territory Name', 'City Code', 'City Name', 'SDG Sub-Region', 'SDG Region', 'Average share of green area in city/urban area 1990 (%)', 'Average share of green area in city/ urban area 2000 (%)', 'Average share of green area in city/ urban area 2010 (%)', 'Average share of green area in city/ urban area 2020 (%)', 'Green area per capita 1990 (m2/person)', 'Green area per capita 2000 (m2/person)', 'Green area per capita 2010 (m2/person)', 'Green area per capita 2020 (m2/person)', 'Data Source', 'FootNote'] \n",
      "\n",
      "Aperçu colonnes normalisées (green):\n",
      "  Country or Territory Name       City Name          City Code country_norm  \\\n",
      "0               Afghanistan           Kabul           AF_KABUL  afghanistan   \n",
      "1               Afghanistan           Herat           AF_HERAT  afghanistan   \n",
      "2               Afghanistan  Mazar-e Sharif  AF_MAZAR_E_SHARIF  afghanistan   \n",
      "3               Afghanistan        Kandahar        AF_KANDAHAR  afghanistan   \n",
      "4                   Algeria          Annaba          DZ_ANNABA      algeria   \n",
      "\n",
      "        city_norm      citycode_norm country_join  \n",
      "0           kabul           af_kabul  afghanistan  \n",
      "1           herat           af_herat  afghanistan  \n",
      "2  mazar-e sharif  af_mazar_e_sharif  afghanistan  \n",
      "3        kandahar        af_kandahar  afghanistan  \n",
      "4          annaba          dz_annaba      algeria   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger green areas (CSV ou XLSX)\n",
    "\n",
    "df_green = pd.read_csv(GREEN_CSV_PATH, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"=== Green areas (raw) ===\")\n",
    "print(\"Nb de lignes :\", len(df_green))\n",
    "print(\"Colonnes :\", list(df_green.columns), \"\\n\")\n",
    "\n",
    "# Renommer quelques colonnes clés pour simplifier\n",
    "# (adapte les noms si pandas a fait des trucs bizarres)\n",
    "col_country = \"Country or Territory Name\"\n",
    "col_citycode = \"City Code\"\n",
    "col_cityname = \"City Name\"\n",
    "col_share2020 = \"Average share of green area in city/ urban area 2020 (%)\"\n",
    "col_percap2020 = \"Green area per capita 2020 (m2/person)\"\n",
    "\n",
    "# Normalisation pays / villes / code ville\n",
    "df_green[\"country_norm\"] = df_green[col_country].apply(normalize_str)\n",
    "df_green[\"city_norm\"] = df_green[col_cityname].apply(normalize_str)\n",
    "df_green[\"citycode_norm\"] = df_green[col_citycode].apply(normalize_str)\n",
    "df_green[\"country_join\"] = df_green[\"country_norm\"].apply(harmonize_country_norm)\n",
    "\n",
    "print(\"Aperçu colonnes normalisées (green):\")\n",
    "print(df_green[[col_country, col_cityname, col_citycode, \"country_norm\", \"city_norm\", \"citycode_norm\", \"country_join\"]].head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83a83d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== A) Doublons GREEN AREAS ===\n",
      "\n",
      "Nb de lignes avec city_norm dupliqué : 14\n",
      "Nb de lignes avec (country_join, city_norm) dupliqués : 0\n",
      "Nb de lignes avec citycode_norm dupliqué : 8\n",
      "\n",
      "Top 10 City Names ambigus (nb de pays distincts) :\n",
      "city_norm\n",
      "hyderabad              2\n",
      "london                 2\n",
      "tarabulus (tripoli)    2\n",
      "Name: country_join, dtype: int64 \n",
      "\n",
      "Doublons City Code (hors NA) = 0\n"
     ]
    }
   ],
   "source": [
    "# A) Doublons green areas\n",
    "\n",
    "print(\"=== A) Doublons GREEN AREAS ===\")\n",
    "\n",
    "# 1) Doublons sur City Name seul (normalisé)\n",
    "dups_city = df_green[df_green.duplicated(subset=[\"city_norm\"], keep=False)]\n",
    "print(\"\\nNb de lignes avec city_norm dupliqué :\", len(dups_city))\n",
    "\n",
    "# 2) Doublons sur (Country+City)\n",
    "dups_country_city = df_green[df_green.duplicated(subset=[\"country_join\", \"city_norm\"], keep=False)]\n",
    "print(\"Nb de lignes avec (country_join, city_norm) dupliqués :\", len(dups_country_city))\n",
    "\n",
    "# 3) Doublons sur City Code\n",
    "dups_citycode = df_green[df_green.duplicated(subset=[\"citycode_norm\"], keep=False)]\n",
    "print(\"Nb de lignes avec citycode_norm dupliqué :\", len(dups_citycode))\n",
    "\n",
    "# 4) Top 10 City Names ambigus (présents dans plusieurs pays)\n",
    "ambig = (\n",
    "    df_green.groupby(\"city_norm\")[\"country_join\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "ambig = ambig[ambig > 1].head(10)\n",
    "print(\"\\nTop 10 City Names ambigus (nb de pays distincts) :\")\n",
    "print(ambig, \"\\n\")\n",
    "\n",
    "# doublons de City Code en ignorant les valeurs manquantes\n",
    "tmp = df_green[df_green[\"citycode_norm\"].notna()]\n",
    "dups_citycode_real = tmp[tmp.duplicated(subset=[\"citycode_norm\"], keep=False)]\n",
    "print(\"Doublons City Code (hors NA) =\", len(dups_citycode_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b42d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== B) Valeurs manquantes ===\n",
      "\n",
      "Green areas - nb de valeurs manquantes :\n",
      "Country or Territory Name                                   0\n",
      "City Name                                                   8\n",
      "City Code                                                   8\n",
      "Average share of green area in city/ urban area 2020 (%)    0\n",
      "Green area per capita 2020 (m2/person)                      0\n",
      "dtype: int64\n",
      "\n",
      "Pollution JSON - nb de valeurs manquantes :\n",
      "country_name       0\n",
      "city_name          0\n",
      "aqi.value          0\n",
      "iaqi.pm25.value    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# B) Valeurs manquantes\n",
    "\n",
    "print(\"=== B) Valeurs manquantes ===\")\n",
    "\n",
    "# GREEN AREAS : remplacer chaînes vides par NaN pour la détection\n",
    "cols_check_green = [\n",
    "    col_country,\n",
    "    col_cityname,\n",
    "    col_citycode,\n",
    "    col_share2020,\n",
    "    col_percap2020,\n",
    "]\n",
    "\n",
    "df_green[cols_check_green] = df_green[cols_check_green].replace(\n",
    "    r\"^\\s*$\", pd.NA, regex=True\n",
    ")\n",
    "\n",
    "print(\"\\nGreen areas - nb de valeurs manquantes :\")\n",
    "print(df_green[cols_check_green].isna().sum())\n",
    "\n",
    "# POLLUTION : vérifier quelques colonnes importantes\n",
    "cols_check_poll = [\n",
    "    \"country_name\",\n",
    "    \"city_name\",\n",
    "    \"aqi.value\",\n",
    "    \"iaqi.pm25.value\",\n",
    "]\n",
    "\n",
    "print(\"\\nPollution JSON - nb de valeurs manquantes :\")\n",
    "print(df_poll[cols_check_poll].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e59da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== C) Doublons POLLUTION ===\n",
      "Nb de lignes pollution avec city_norm dupliqué : 0\n",
      "Nb de lignes pollution avec (country_join, city_norm) dupliqués : 0\n"
     ]
    }
   ],
   "source": [
    "# C) Doublons aussi côté pollution (pour info)\n",
    "\n",
    "print(\"\\n=== C) Doublons POLLUTION ===\")\n",
    "\n",
    "dups_poll_city = df_poll[df_poll.duplicated(subset=[\"city_norm\"], keep=False)]\n",
    "print(\"Nb de lignes pollution avec city_norm dupliqué :\", len(dups_poll_city))\n",
    "\n",
    "dups_poll_country_city = df_poll[df_poll.duplicated(subset=[\"country_join\", \"city_norm\"], keep=False)]\n",
    "print(\"Nb de lignes pollution avec (country_join, city_norm) dupliqués :\", len(dups_poll_country_city))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93c7703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Résumé rapide ===\n",
      "Green areas : 667 lignes\n",
      "Pollution   : 23035 lignes\n",
      "-> Voir les stats ci-dessus pour les doublons et valeurs manquantes.\n"
     ]
    }
   ],
   "source": [
    "# Résumé rapide\n",
    "\n",
    "print(\"\\n=== Résumé rapide ===\")\n",
    "print(f\"Green areas : {len(df_green)} lignes\")\n",
    "print(f\"Pollution   : {len(df_poll)} lignes\")\n",
    "print(\"-> Voir les stats ci-dessus pour les doublons et valeurs manquantes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "083e62d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green pairs: 667\n",
      "Poll pairs : 23035\n",
      "Common     : 365\n",
      "[('india', 'anand'), ('france', 'lille'), ('argentina', 'san juan'), ('afghanistan', 'herat'), ('democratic republic of the congo', 'kisangani'), ('ukraine', 'odesa'), ('poland', 'legnica'), ('spain', 'burgos'), ('niger', 'niamey'), ('brazil', 'salvador'), ('honduras', 'san pedro sula'), ('united states', 'raleigh'), ('saudi arabia', 'jiddah'), ('senegal', 'ziguinchor'), ('pakistan', 'larkana'), ('namibia', 'windhoek'), ('malaysia', 'ipoh'), ('china', 'zhuji'), ('united states', 'las vegas'), ('india', 'amritsar')]\n",
      "Exemples green non trouvés dans pollution: [('turkiye', 'ankara'), ('germany', 'oldenburg (oldenburg)'), ('saudi arabia', 'ad-dammam'), ('iran (islamic republic of)', 'nishabur (nishapur/neyshabur)'), ('tunisia', 'susah (sousse)'), ('dominican republic', 'santo domingo'), ('luxembourg', 'luxembourg (letzebuerg)'), ('tajikistan', 'istaravsan (istarawshan)'), ('brazil', 'parauapebas'), ('morocco', 'sidi slimane'), ('turkiye', 'carsamba'), ('algeria', 'tiaret (tihert)'), ('syrian arab republic', 'al-hasakah'), ('denmark', 'københavn (copenhagen)'), ('chile', 'la serena-coquimbo'), ('india', 'meerut'), ('spain', 'granada'), ('timor-leste', 'dili'), ('china', 'yulin, guangxi'), ('india', 'mumbai (bombay)')]\n"
     ]
    }
   ],
   "source": [
    "pairs_green = set(zip(df_green[\"country_join\"], df_green[\"city_norm\"]))\n",
    "pairs_poll  = set(zip(df_poll[\"country_join\"],  df_poll[\"city_norm\"]))\n",
    "\n",
    "common = pairs_green & pairs_poll\n",
    "print(\"Green pairs:\", len(pairs_green))\n",
    "print(\"Poll pairs :\", len(pairs_poll))\n",
    "print(\"Common     :\", len(common))\n",
    "\n",
    "# exemples de matches\n",
    "print(list(common)[:20])\n",
    "\n",
    "# si common est faible, voir ce qui manque côté green\n",
    "only_green = pairs_green - pairs_poll\n",
    "print(\"Exemples green non trouvés dans pollution:\", list(only_green)[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
